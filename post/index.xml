<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Paul&#39;s blog</title>
    <link>https://blog.paulhankin.net/post/</link>
    <description>Recent content in Posts on Paul&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.paulhankin.net/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A suspiciously fast program</title>
      <link>https://blog.paulhankin.net/powercheat/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/powercheat/</guid>
      <description>&lt;p&gt;Computing large integer powers modulo some number is a somewhat common operation. For example, it&#39;s used in RSA encryption. Usually, this is done using exponentiation by squaring, but this go program correctly prints the results of &lt;span  class=&#34;math&#34;&gt;\(n^{2^{64}}\ (\mathrm{mod}\ 115763)\)&lt;/span&gt; for &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt; from 1 to 20, seemingly naively:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

func main() {
	for n := 1; n &amp;lt;= 20; n++ {
		result := 1
		for i := 0; i &amp;lt; 2^64; i++ {
			result = (result * n) % 115763
		}
		fmt.Printf(&amp;quot;pow(%d, pow(2, 64)) mod 115763 = %d\n&amp;quot;, n, result)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It runs, unoptimized, in a few milliseconds on my desktop.
You can run it yourself online using the &lt;a href=&#34;https://play.golang.org/p/y2L63tDfUMJ&#34;&gt;go playground&lt;/a&gt;. Feel free to edit the code a little before running it to convince yourself it&#39;s not just fast because the playground is caching the results or something.&lt;/p&gt;

&lt;p&gt;How can it be that fast? Is go&#39;s optimizing compiler that clever? It&#39;s not, and there&#39;s a trick in the code. Can you see it?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generating random Latin squares</title>
      <link>https://blog.paulhankin.net/latinsquares/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/latinsquares/</guid>
      <description>&lt;p&gt;Generation of random Latin Squares (such that each latin square of a given size is equally
likely) is a deceptively difficult problem.&lt;/p&gt;

&lt;p&gt;This post describes my own implementation, loosely based on the &lt;a href=&#34;https://www.researchgate.net/publication/308517970_Generation_of_Random_Latin_Squares_Step_By_Step_and_Graphically&#34;&gt;Java implementation described by Ignacio Gallego Sagastume&lt;/a&gt; which
implements the rather ingenious method of &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291520-6610%281996%294%3A6%3C405%3A%3AAID-JCD3%3E3.0.CO%3B2-J&#34;&gt;Jacobson and Matthews&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A gentle introduction to hard programming</title>
      <link>https://blog.paulhankin.net/learnprogramming/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/learnprogramming/</guid>
      <description>&lt;p&gt;As I was growing up in England in the 80s, there was a boom in
home microcomputers, with the Commodore 64, the ZX Spectrum,
and the BBC Micro being three popular choices. These
provided an excellent and approachable introduction to programming,
with many of my friends learning programming in BASIC and assembler.
We taught ourselves the fundamentals of computing while we were playing,
and at a relatively early age.&lt;/p&gt;

&lt;p&gt;These days the computing environment is complex, and it&#39;s much harder for a
beginner to get started, or even know how to get started. Mostly
programming is learnt at university or in other formal education.
While there is definitely more to learn now than before, it seems like
the fundamentals of coding should still be easier to pick up than
it currently is.&lt;/p&gt;

&lt;p&gt;This post takes a look at what made home micros effective
learning environments, and considers what a modern equivalent might look
like.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Insurance and the Kelly criterion</title>
      <link>https://blog.paulhankin.net/kellycriterion/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/kellycriterion/</guid>
      <description>&lt;p&gt;This article describes how to use the Kelly criterion to make rational
choices when confronted with a risky financial decision, and suggests
a way to estimate the most you should be willing to pay for any
particular sort of insurance.&lt;/p&gt;

&lt;p&gt;The Kelly criterion (which at its core is the idea that the logarithm
of your wealth is a better measure of money&#39;s value to you than its absolute
value) is well understood by the informed gambling community, and
should be more widely known.&lt;/p&gt;

&lt;p&gt;If you decide to apply the knowledge in this post, also consult with a financial
professional (which as we&#39;ll see later doesn&#39;t include most finance or economics
students, and most young financial professionals), and read the disclaimer at the end.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A novel and efficient way to compute Fibonacci numbers</title>
      <link>https://blog.paulhankin.net/fibonacci2/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/fibonacci2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://blog.paulhankin.net/fibonacci/&#34;&gt;An earlier post&lt;/a&gt; described how to compute Fibonacci numbers in a single arithmetic expression.&lt;/p&gt;

&lt;p&gt;Far√© Rideau, the author of a &lt;a href=&#34;http://fare.tunes.org/files/fun/fibonacci.lisp&#34;&gt;page of Fibonacci computations in Lisp&lt;/a&gt;, suggested in a private
email a simple and efficient variant, that I believe is novel.&lt;/p&gt;

&lt;p&gt;For &lt;span  class=&#34;math&#34;&gt;\(X\)&lt;/span&gt; large enough, &lt;span  class=&#34;math&#34;&gt;\(\mathrm{Fib}_n = (X^{n+1}\ \mathrm{mod}\ (X^2-X-1))\ \mathrm{mod}\ X\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;That means you can compute Fibonacci numbers efficiently with a simple program:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for n in range(1, 21):
    X = 1&amp;lt;&amp;lt;(n+2)
    print(pow(X, n+1, X*X-X-1) % X)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This blog post describes how this method works, gives a few ways to think about it, easily infers the fast Fibonacci doubling formulas, provides a nice alternative to Binet&#39;s formula relating the golden ratio and Fibonacci numbers, and expands the method to generalized Fibonacci recurrences, including a near one-line solution to the problem of counting how many ways to reach the end-square of a 100-square game using a single six-sided dice.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Little Man Computer</title>
      <link>https://blog.paulhankin.net/littlemancomputer/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/littlemancomputer/</guid>
      <description>&lt;p&gt;I had never seen this mini-assembler-based educational computer before. &lt;a href=&#34;https://en.wikipedia.org/wiki/Little_man_computer&#34;&gt;wikipedia.org/Little_man_computer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I couldn&#39;t find a good online emulator, so I wrote one: &lt;a href=&#34;https://blog.paulhankin.net/lmc/lmc.html&#34;&gt;Little Man Computer Emulator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Enter the program on the left, click &amp;quot;Assemble&amp;quot;, enter some inputs if your program needs them, and then step
through the execution.&lt;/p&gt;

&lt;p&gt;It&#39;s probably got some bugs since it was a quick hack, but it worked on the examples I tried it on.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Near-optimal closed-hand Chinese Poker.</title>
      <link>https://blog.paulhankin.net/chinesepoker/</link>
      <pubDate>Thu, 21 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/chinesepoker/</guid>
      <description>&lt;p&gt;This blog post looks at closed-hand Chinese Poker, and describes
a near-optimal strategy for it which is readily implementable
on a computer.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Everything you know about complexity is wrong</title>
      <link>https://blog.paulhankin.net/complexity-rant/</link>
      <pubDate>Wed, 06 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/complexity-rant/</guid>
      <description>&lt;p&gt;Who would disagree that the run-time of mergesort is &lt;span  class=&#34;math&#34;&gt;\(O(n\mathrm{log}\,n)\)&lt;/span&gt; and it&#39;s asymptotically optimal?
Not many programmers I reckon, except perhaps to question whether it&#39;s talking about
a model of computation that&#39;s not sufficiently close to a real computer, for example a quantum
computer or one that performs arbitrary operations in parallel (possibly
involving &lt;a href=&#34;http://en.wikipedia.org/wiki/Spaghetti_sort&#34;&gt;sticks of spaghetti&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;However, if you try to understand how to formalize what it means for a sort
to run in &lt;span  class=&#34;math&#34;&gt;\(O(n\mathrm{log}\,n)\)&lt;/span&gt; and for it to be optimal,
it&#39;s surprisingly difficult to find a suitable computational model, that is,
an abstraction of a computer which elides all but the important
details of the computer: the operations it can perform, and how the memory
works.&lt;/p&gt;

&lt;p&gt;In this post, I&#39;ll look at some of
the most common computational models used in both practice and theory, and
find out that they&#39;re all flawed in one way or another, and in fact in all
of them either mergesort doesn&#39;t run in &lt;span  class=&#34;math&#34;&gt;\(O(n\mathrm{log}\,n)\)&lt;/span&gt; or there&#39;s
asymptotically faster sorts.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An integer formula for Fibonacci numbers</title>
      <link>https://blog.paulhankin.net/fibonacci/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.paulhankin.net/fibonacci/</guid>
      <description>&lt;p&gt;This code, somewhat surprisingly, generates Fibonacci numbers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def fib(n):
    return (4 &amp;lt;&amp;lt; n*(3+n)) // ((4 &amp;lt;&amp;lt; 2*n) - (2 &amp;lt;&amp;lt; n) - 1) &amp;amp; ((2 &amp;lt;&amp;lt; n) - 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this blog post, I&#39;ll explain where it comes from and how it works.
&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>